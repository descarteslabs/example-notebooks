{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d967e95b-bfe8-4227-9f07-5f2e731fcf88",
   "metadata": {},
   "source": [
    "# Computer Vision on the Descartes Labs Platform - Training a Simple UNet Model\n",
    "__________________\n",
    "\n",
    "This notebook will demonstrate how one can utilize Descartes Labs Workbench to efficiently prototype and iterate on training an image segmentation model. This is meant to serve _solely as a jumping off point_ and is not intended to be used as a panacea for all machine learning needs. \n",
    "\n",
    "#### *__Note: It is STRONGLY urged that you use a GPU-enabled environment to execute this notebook__*\n",
    "\n",
    "The general outline of this sample is as follows:\n",
    "* Load in the training data generated in [03a Generate Training Data.ipynb](03a%20Generate%20Training%20Data.ipynb), saved locally as .npy files\n",
    "* Train and compile a simple convolutional neural network with a U-Net architecture using [TensorFlow](https://www.tensorflow.org/guide/keras) \n",
    "    * *Note: This should not be seen as an end-all deep learning model, this is an extremely simplified example focused on the general patterns over accuracy*\n",
    "* Save our trained segmentation model as a [`Blob`](https://docs.descarteslabs.com/descarteslabs/catalog/docs/blob.html#descarteslabs.catalog.Blob) to infer asynchronously using [`Batch Compute`](https://docs.descarteslabs.com/descarteslabs/compute/readme.html) in [03c Deploying a Segmentation Model.ipynb](03c%20Deploying%20a%20Segmentation%20Model.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4fbcfb1-d643-469e-9253-956f269d61d5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import descarteslabs as dl\n",
    "from descarteslabs.catalog import Blob, properties as p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30de0bc6-e5c4-4d5b-ae5b-3931ffea92c5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pickle, os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbcad146-7ce6-4dbe-83c0-92b3d706efb4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "from tensorflow.keras.layers import (\n",
    "    Input,\n",
    "    Conv2D,\n",
    "    MaxPooling2D,\n",
    "    UpSampling2D,\n",
    "    Concatenate,\n",
    ")\n",
    "from tensorflow.keras.models import Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "255a6aa6-f5f4-47ec-a4a3-88ef96690b03",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "Load in our data and mask arrays from the previous notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6716f982-3bca-4c02-8438-98095e1069ed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_array = np.load(\"data_array.npy\")\n",
    "mask_array = np.load(\"mask_array.npy\")\n",
    "data_array.shape, mask_array.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7099794a-c282-4ec1-8fc4-53288ed99b12",
   "metadata": {},
   "source": [
    "Plot a single pair:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62b4cad3-e94d-48c9-b65d-3f75266e19e8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=2)\n",
    "ax[0].imshow(data_array[0] / 255)\n",
    "ax[1].imshow(mask_array[0, :, :, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cda88d10-2561-4564-8625-a73524dba2c0",
   "metadata": {},
   "source": [
    "Very simply 80:20 train/test split:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e6a055b-4995-4d68-b3d7-50eb08e4cd88",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_data = data_array[:800]\n",
    "val_data = data_array[800:]\n",
    "train_masks = mask_array[:800]\n",
    "val_masks = mask_array[800:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f38c5f46-0b5a-49f2-857e-43eb9c7e41b5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_data.shape, val_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f68c2ab2-f638-416c-9fd4-7e2e39fed1ed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_masks.shape, val_masks.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b25c54d-c156-4304-8cda-7fe054c0f198",
   "metadata": {},
   "source": [
    "## Training a Computer Vision Model\n",
    "A sample U-Net architecture, commonly used in image segmentation tasks. In practice this is where you should input your own methodology and iterate!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad5e8325-e2ad-44a5-a0e5-965758629543",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomUNet:\n",
    "    def __init__(\n",
    "        self, optimizer=\"adam\", activation=\"sigmoid\", loss=\"binary_crossentropy\"\n",
    "    ):\n",
    "        self.optimizer = optimizer\n",
    "        self.activation = activation\n",
    "        self.loss = loss\n",
    "        self.model = self.build_model()\n",
    "\n",
    "    def build_model(self):\n",
    "        # Define input layer\n",
    "        inputs = Input(shape=(None, None, 3))\n",
    "\n",
    "        # Encoder\n",
    "        conv1 = Conv2D(64, 3, activation=\"relu\", padding=\"same\")(inputs)\n",
    "        pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "        conv2 = Conv2D(128, 3, activation=\"relu\", padding=\"same\")(pool1)\n",
    "        pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "        conv3 = Conv2D(256, 3, activation=\"relu\", padding=\"same\")(pool2)\n",
    "        pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
    "        conv4 = Conv2D(512, 3, activation=\"relu\", padding=\"same\")(pool3)\n",
    "        pool4 = MaxPooling2D(pool_size=(2, 2))(conv4)\n",
    "\n",
    "        # Center\n",
    "        conv5 = Conv2D(1024, 3, activation=\"relu\", padding=\"same\")(pool4)\n",
    "\n",
    "        # Decoder\n",
    "        up6 = Concatenate()([UpSampling2D(size=(2, 2))(conv5), conv4])\n",
    "        conv6 = Conv2D(512, 3, activation=\"relu\", padding=\"same\")(up6)\n",
    "        up7 = Concatenate()([UpSampling2D(size=(2, 2))(conv6), conv3])\n",
    "        conv7 = Conv2D(256, 3, activation=\"relu\", padding=\"same\")(up7)\n",
    "        up8 = Concatenate()([UpSampling2D(size=(2, 2))(conv7), conv2])\n",
    "        conv8 = Conv2D(128, 3, activation=\"relu\", padding=\"same\")(up8)\n",
    "        up9 = Concatenate()([UpSampling2D(size=(2, 2))(conv8), conv1])\n",
    "        conv9 = Conv2D(64, 3, activation=\"relu\", padding=\"same\")(up9)\n",
    "\n",
    "        # Output layer\n",
    "        outputs = Conv2D(1, 1, activation=self.activation)(conv9)\n",
    "\n",
    "        # Define the model\n",
    "        model = Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "        # Compile the model\n",
    "        model.compile(optimizer=self.optimizer, loss=self.loss, metrics=[\"accuracy\"])\n",
    "\n",
    "        return model\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "unet_model = CustomUNet()\n",
    "model = unet_model.model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb22b9bc-c602-4dfd-8844-7a72c072299f",
   "metadata": {},
   "source": [
    "### _Note: This will take extremely long if you are not using a GPU-enabled environment!_\n",
    "\n",
    "Here we fit our model. On the GPU-enabled Descartes Labs Workbench this step takes roughly 35 minutes to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a27b78c1-23e4-41a3-b861-0bcb087edd0d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.fit(train_data, train_masks, epochs=100, validation_data=(val_data, val_masks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9935cf77-8cc3-4178-8952-871bfbd878ce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d2becbd-c9ee-44ef-8798-5902f19b7a83",
   "metadata": {},
   "source": [
    "## Testing Predictions\n",
    "Inferring our model over our validation data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0344c497-5574-4408-b4e0-e77e021284e5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "preds = model.predict(val_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0138ad2-c268-453b-bb9f-c5a59fda8cd2",
   "metadata": {},
   "source": [
    "And visualizing one result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "080df31d-d2d8-4e36-b7cd-632e475333a7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=2)\n",
    "ax[0].imshow(val_data[0] / 255)\n",
    "ax[1].imshow(preds[0, :, :, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb5dab6b-9763-4311-b392-727d10b95fa3",
   "metadata": {},
   "source": [
    "## Saving for Scale\n",
    "At this point, you should again be iterating in the real world. Assuming we are happy with the performance of our model, we can next save it as a static file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8601d9b-a93f-48e9-91fd-769b545bc1d8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.save(\"training_segmentation.keras\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d12eee00-ef50-425a-b72b-e7d881499506",
   "metadata": {},
   "source": [
    "Here we save as a blob to reference in [03c Deploying a Segmentation Model.ipynb](03c%20Deploying%20a%20Segmentation%20Model.ipynb):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da6fd19-cf8c-428b-8049-3c919209621d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "org = dl.auth.Auth().payload[\"org\"]\n",
    "user_hash = dl.auth.Auth().namespace\n",
    "namespace = f\"{org}:{user_hash}\" if org else user_hash\n",
    "namespace"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1255228-0c0d-4949-8220-f4275582a670",
   "metadata": {},
   "source": [
    "Note, you don't always need to delete and overwrite your blobs such as in the following cell. This is an example where we do not care about deleting previous iterations. In practice you should try to preserve, attribute, and version control your model weights files for iteration!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40ce26a0-45c5-4066-8df0-df35fe95afac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    # Create a new Blob object\n",
    "    blob = Blob(\n",
    "        name=\"training_segmentation\",\n",
    "        tags=[\"deep-learning-examples\"],\n",
    "    )\n",
    "    # Upload our model to this Blob:\n",
    "    blob.upload(\"training_segmentation.keras\")\n",
    "except:\n",
    "    print(\"Blob already exists, overwriting\")\n",
    "    # Already exists, overwriting\n",
    "    blob = Blob.get(name=\"training_segmentation\", \n",
    "                    namespace=namespace\n",
    "                   )\n",
    "    blob.delete()\n",
    "    print(\"Deleted blob\")\n",
    "    # Create a new Blob object\n",
    "    blob = Blob(\n",
    "        name=\"training_segmentation\",\n",
    "        tags=[\"deep-learning-examples\"],\n",
    "    )\n",
    "    # Upload our model to this Blob:\n",
    "    blob.upload(\"training_segmentation.keras\")\n",
    "blob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21950699-1a3a-4f52-a509-133dcfbce788",
   "metadata": {},
   "source": [
    "Finally cleaning up:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2363fc81-2bc2-417e-a718-47923bb57061",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.remove(\"training_segmentation.keras\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79318d2b-243c-4243-8e46-8de47a4914ff",
   "metadata": {},
   "source": [
    "#### Note: After completing this notebook it is recommended to shut your notebook kernel down before moving on to [03c Deploying a Segmentation Model.ipynb](03c%20Deploying%20a%20Segmentation%20Model.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8064b05f-533b-47ed-905b-a234caf66494",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
